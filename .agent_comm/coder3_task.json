{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.RO_2507.23523v1_H_RDT_Human_Manipulation_Enhanced_Bimanual_Roboti",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.RO_2507.23523v1_H-RDT-Human-Manipulation-Enhanced-Bimanual-Roboti with content analysis. Detected project type: agent (confidence score: 7 matches).",
    "key_algorithms": [
      "Policy",
      "Uniform",
      "Vision-Language-Action",
      "Work",
      "When",
      "Robotic",
      "Language",
      "Machine",
      "Dation",
      "Flow"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.RO_2507.23523v1_H-RDT-Human-Manipulation-Enhanced-Bimanual-Roboti.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nH-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation\nHongzhe Bi1,2, Lingxuan Wu1, Tianwei Lin2, Hengkai Tan1,\nZhizhong Su2, Hang Su1, Jun Zhu1\n1Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab,\nTsinghua-Bosch Joint ML Center, Tsinghua University\n2Horizon Robotics\nbhz24@mails.tsinghua.edu.cn\n H-RDT\n  Dataset\n Human Dataset Robot DatasetsPre-Training Fine-Tuning\n300K+\nEpisodes\nFew-Shot Learning Deformable Object\nTowel Folding \nCross-\nEmbodied\nSuccess Rates\nStack Bowls, etc.\nHuman to Robotics Diffusion Transformer\nDiffusion Transformer 2B\nT5DinoV2\nSigLIPODE Inference\nPretrain Finetune\nFigure 1: Overview of H-RDT. A human-to-robotics diffusion transformer with two-stage training.\nAbstract\nImitation learning for robotic manipulation faces a funda-\nmental challenge: the scarcity of large-scale, high-quality\nrobot demonstration data. Recent robotic foundation mod-\nels often pre-train on cross-embodiment robot datasets to in-\ncrease data scale, while they face significant limitations as\nthe diverse morphologies and action spaces across different\nrobot embodiments make unified training challenging. In this\npaper, we present H-RDT (Human to Robotics Diffusion\nTransformer), a novel approach that leverages human ma-\nnipulation data to enhance robot manipulation capabilities.\nOur key insight is that large-scale egocentric human manip-\nulation videos with paired 3D hand pose annotations pro-\nvide rich behavioral priors that capture natural manipulation\nstrategies and can benefit robotic policy learning. We intro-\nduce a two-stage training paradigm: (1) pre-training on large-\nscale egocentric human manipulation data, and (2) cross-\nembodiment fine-tuning on robot-specific data with modu-\nlar action encoders and decoders. Built on a diffusion trans-\nformer architecture with 2B parameters, H-RDT uses flowmatching to model complex action distributions. The mod-\nular design of action encoder and decoder components en-\nables effective knowledge transfer from the unified human\nembodiment to diverse robot platforms through efficient fine-\ntuning. Extensive evaluations encompassing both simulation\nand real-world experiments, single-task and multitask scenar-\nios, as well as few-shot learning and robustness assessments,\ndemonstrate that H-RDT outperforms training from scratch\nand existing state-of-the-art methods, including \u03c00and RDT,\nachieving significant improvements of 13.9% and 40.5%\nover training from scratch in simulation and real-world exper-\niments, respectively. The results validate our core hypothesis\nthat human manipulation data can serve as a powerful foun-\ndation for learning bimanual robotic manipulation policies.\nSee our project page for code and pretrained models.\nIntroduction\nRecent advances in robotic learning have been driven by\nspecialized action policies like ACT (Zhao et al. 2023),arXiv:2507.23523v1  [cs.RO]  31 Jul 2025\n\n--- Page 2 ---\nDiffusion Policy (Chi et al. 2023), and 3D Diffusion Pol-\nicy (Ze et al. 2024), as well as Vision-Language-Action\n(VLA) models such as RT-2 (Brohan et al. 2023), Open-\nVLA (Kim et al. 2024), RDT (Liu et al. 2024), \u03c00(Black\net al. 2024), and \u03c00.5(Intelligence et al. 2025). However,\nthese approaches face fundamental data collection chal-\nlenges. Robot demonstration data relies heavily on teleoper-\nation (Zhao et al. 2023; Aldaco et al. 2024), which requires\nexpensive equipment and skilled operators, while advanced\ndata collection systems like Universal Manipulation Inter-\nface (Chi et al. 2024) and motion capture setups (Wang et al.\n2024; Xu et al. 2025) suffer from complex infrastructure re-\nquirements and inconsistent data quality that limit scalabil-\nity.\nCurrent VLA models typically employ cross-\nembodiment pre-training on robot datasets like Open\nX-Embodiment (O\u2019Neill et al. 2024) and AgiBot World\nColosseo (Bu et al. 2025). This approach faces two\ncritical limitations: the diverse morphologies and action\nspaces across robot embodiments make unified training\nchallenging, and existing robot datasets remain limited\nin scale with heterogeneous data quality across different\ncollection setups. These constraints fundamentally limit the\ndata availability and generalization capabilities needed for\ngeneral-purpose robotic manipulation (Team et al. 2024;\nO\u2019Neill et al. 2024).\nIn stark contrast, human manipulation behaviors repre-\nsent a vast, readily accessible repository of demonstration\ndata. The recent emergence of large-scale egocentric video\ndatasets with detailed hand pose annotations, exemplified by\nEgoDex (Hoque et al. 2025) with its 829 hours of manipu-\nlation videos, offers unprecedented opportunities for learn-\ning rich behavioral priors. Human demonstrations naturally\ncapture object affordances, manipulation strategies, and task\ndecomposition patterns that could potentially serve as pow-\nerful inductive biases for robotic learning. Recent works\nhave begun exploring this direction: EgoMimic (Kareer et al.\n2024) employs co-training on human and robot data with\negocentric video, while Humanoid Policy (HAT) (Qiu et al.\n2025) uses differentiable retargeting for human-humanoid\nbehavior modeling.\nThis paper introduces H-RDT (Human to Robotics Dif-\nfusion Transformer), a novel approach that systematically\nleverages large-scale egocentric human manipulation data to\nenhance robot manipulation capabilities. Our approach fo-\ncuses on three specific aspects: Data Scarcity: We harness\nthe abundance of human manipulation videos with 3D hand\npose annotations to provide rich behavioral priors that cap-\nture natural manipulation strategies, object affordances, and\ntask decomposition patterns. Cross-Embodiment Trans-\nfer: We develop a modular transformer architecture with\nspecialized action encoders and decoders that enable ef-\nfective knowledge transfer from human demonstrations to\ndiverse robotic platforms while preserving learned ma-\nnipulation knowledge. Training Efficiency: We employ\na two-stage training paradigm with flow matching, first\npre-training on large-scale human data followed by cross-\nembodiment fine-tuning, enabling stable and efficient policy\nlearning throughout.Our method introduces structural and training method in-\nnovations for human-to-robot knowledge transfer through\nhuman manipulation data pre-training.\nThe main contributions of this work are:\n\u2022 A novel framework for systematically leveraging large-\nscale egocentric human manipulation data to enhance\nrobotic policy learning\n\u2022 A diffusion transformer architecture with modular\nhuman-to-robot transfer components that enables effec-\ntive cross-embodiment knowledge transfer\n\u2022 A comprehensive empirical validation demonstrating\nconsistent improvements over state-of-the-art methods\nacross simulation and real-world scenarios\n\u2022 Insights into the value of human manipulation priors for\nsample-efficient robot learning, particularly in few-shot\nsettings\nRelated Work\nLearning-based Robotic Manipulation\nRecent advances in imitation learning have been driven by\nspecialized action policies including ACT (Zhao et al. 2023),\nDiffusion Policy (Chi et al. 2023), and 3D Diffusion Pol-\nicy (Ze et al. 2024). These action policies focus on learn-\ning direct visuomotor control for manipulation tasks, show-\ning promising results on dexterous manipulation through ad-\nvanced sequence modeling and generative approaches.\nThe emergence of Vision-Language-Action (VLA) mod-\nels represents a significant paradigm shift toward more gen-\neralizable robotic systems. Recent VLA approaches include\nRT-2 (Brohan et al. 2023), OpenVLA (Kim et al. 2024), the\nRobotics Diffusion Transformer (RDT) (Liu et al. 2024),\n\u03c00(Black et al. 2024), \u03c00.5(Intelligence et al. 2025), and\nother VLA models (Zhao et al. 2025; Zhen et al. 2024; Liu\net al. 2025; Wen et al. 2025b,a). These models combine\nvisual understanding, language comprehension, and action\ngeneration within unified architectures, enabling instruction-\nfollowing capabilities and cross-embodiment generalization\nthrough large-scale datasets (O\u2019Neill et al. 2024; Wu et al.\n2024; Khazatsky et al. 2024; Fang et al. 2023).\nOur work builds upon the RDT architecture while in-\ntroducing novel structural and training method innovations.\nSpecifically, we adopt flow matching (Lipman et al. 2022;\nLiu 2022) as our training paradigm, which offers improved\nstability and efficiency compared to traditional diffusion\ntraining (Esser et al. 2024; Bao et al. 2023). More impor-\ntantly, we introduce novel human-to-robot knowledge trans-\nfer mechanisms that enable large-scale pre-training on hu-\nman manipulation data followed by cross-embodiment fine-\ntuning.\nLearning from Egocentric Human Manipulation\nLarge-scale egocentric datasets (Grauman et al. 2022;\nDamen et al. 2018; Liu et al. 2022; Banerjee et al. 2025;\nGrauman et al. 2024) contain tens to hundreds of hours cap-\nturing human-object interaction, yet lack precise 3D hand-\npose annotations required for dexterous manipulation learn-\ning. EgoDex (Hoque et al. 2025) addresses this gap by pro-\n\n--- Page 3 ---\nviding 829 hours (338k episodes) of egocentric video with\nper-frame 3D hand poses and language descriptions.\nEgoMimic (Kareer et al. 2024) and Humanoid Policy\n(HAT) (Qiu et al. 2025) pioneer the use of egocentric hu-\nman videos, yet both operate at modest scales: EgoMimic\ntrains on 2k human demos, and HAT on 27k demos\u2014orders\nof magnitude smaller than the 338k trajectories (829h) em-\nployed by H-RDT. Moreover, these works target a single hu-\nmanoid embodiment; EgoMimic requires paired robot data\nduring co-training, while HAT\u2019s retargeting assumes hu-\nmanoid kinematics. H-RDT, in contrast, decouples large-\nscale human pre-training from robot-specific fine-tuning and\ngeneralizes to arbitrary robot morphologies via modular ac-\ntion adapters. Additional studies explore data augmentation\ntechniques (Li et al. 2025) and paired human\u2013robot data col-\nlection (Xie et al. 2025).\nMethod\nIn this section, we present H-RDT (Human to Robotics Dif-\nfusion Transformer), a novel approach for leveraging large-\nscale human manipulation data to enhance robotic policy\nlearning.\nProblem Formulation\nWe formulate robotic manipulation as a conditional se-\nquence generation problem where the goal is to learn\na policy \u03c0\u03b8that generates action sequences at:t+H=\n{at,at+1, . . . ,at+H\u22121}given multimodal observations.\nFormally, at each timestep t, the agent observes visual ob-\nservations ot\u2208RH\u00d7W\u00d73from one or more RGB cameras,\nproprioceptive state st\u2208Rdsencoding current robot state\nand gripper state, and language instruction l\u2208RL\u00d7dlangde-\nscribing the task. The policy outputs a sequence of future\nactions at:t+Hwhere each action ai\u2208Rdarepresents robot\ncontrol commands (e.g., joint positions, end-effector poses)\nover a prediction horizon H.\nTo achieve a generalist policy, large-scale imitation learn-\ning is required, while the data for a specified embodiment\nare scarce. To counter this, current methods majorly turn to\ntrain with demonstrations from multiple heterogeneous em-\nbodiments (Liu et al. 2024; Black et al. 2024; Intelligence\net al. 2025). However, the total scale of the data remains\nlimited due to the high cost of teleoperation.\nAn alternative approach is to leverage egocentric hu-\nman manipulation data, which could potentially provide data\nfrom a unified human embodiment with manipulation priors\ntransferable across diverse robot platforms, thereby reducing\nthe conflicts of learning from heterogeneous embodiments\nwhile enabling low-cost data acquisition. However, this ap-\nproach faces three main challenges: Firstly, existing meth-\nods operate at modest scales with limited human manipu-\nlation data, failing to fully exploit the potential of human\nbehavioral priors for robotic learning. Secondly, the signif-\nicant embodiment differences between humans and robots,\nincluding end effector types and forward kinematics (Qiu\net al. 2025), make it challenging to effectively transfer ma-\nnipulation knowledge from human demonstrations to target\nrobots. Thirdly, while concurrent work enables manipulationon specific robots using paired human data (Kareer et al.\n2024; Qiu et al. 2025), it remains largely under-addressed\nhow to build a foundation model that can be efficiently\nadapted to multiple diverse robot embodiments through fine-\ntuning on robot-specific data.\nOverview\nTo address the aforementioned challenges, we propose H-\nRDT (Human to Robotics Diffusion Transformer), as illus-\ntrated in Figure 2, a transformer-based architecture trained\nwith a structured paradigm to learn from human data. To\ncounter the embodiment mismatch between humans and\nrobots, H-RDT builds upon a shared action representation\nspace to bridge human and robot embodiments, and satisfies\nthe need for scalable cross-embodiment deployment by uti-\nlizing a two-stage training paradigm. Finally, H-RDT lever-\nages flow matching and a scalable Transformer-based archi-\ntecture for stable and expressive policy learning.\nHuman Action Representation Design\nTo address the challenge of embodiment differences be-\ntween humans and robots, current methods either use flow\nas a transit representation for action (Xu et al. 2024; Wen\net al. 2024), which provides only high-level object motion\nguidance without explicit action parameters and requires\nadditional policy networks to translate flow into robot-\nspecific controls, or require detailed re-targeting between\nhuman pose and target robot, which constrains the applica-\nbility of learning policy. To this end, we utilize detailed 3D\nhand poses, where actions are represented as compact 48-\ndimensional vectors capturing essential bimanual dexterous\ninformation:\n\u2022 Bilateral wrist poses (position (3D) and orientation (6D)\nfor both hands): 18 dimensions, which are identical to the\nEnd-Effector pose of robots\n\u2022 Fingertip positions (3D coordinates for all fingers on both\nhands): 30 dimensions\nThis representation serves as a superset for the action\nspace of most current robots controlled with the End-\nEffector poses, thereby ensuring effective knowledge distil-\nlation across distinct kinematic structures. This structured\nencoding explicitly represents fundamental manipulation\ndynamics and spatial relationships crucial for generalizable\nmanipulation, effectively mitigating embodiment discrepan-\ncies by focusing on universally transferable features such as\ngrasp configurations, orientation constraints, and relative po-\nsitional dynamics.\nTwo-Stage Training Paradigm\nConcurrent work that learns robot policy from human data\nmostly requires rigorous pairing relationship between hu-\nman and target embodiment, thereby failing to adapt to mul-\ntiple embodiments during deployment. To solve this issue,\nwe adopt a carefully designed two-stage training paradigm\nthat maximizes the benefit of human demonstration data\nwhile enabling effective cross-embodiment robot deploy-\nment. Rather than a traditional diffusion objective, H-RDT\n\n--- Page 4 ---\nH-RDT\nBackbone\nHuman Decoder\nDual-UR5\nDecoderDual-Franka\nDecoderAloha-Agilex2\nDecoderDual-ARX5\nDecoder\nStage 1\uff1a Human Pr etrain Stage 2\uff1a Cross-Embodiment  Finetune\nFFNLang Cr oss-AttnImage Cr oss-AttnGQA\nLang EncoderDinoV2 & SigLip\nT5-XXLHuman EncoderState\nAdapterAction\nAdapter\nStateStateNoisy ActionNoisy Action\nFourier  MLP Time EmbedderVision Encoder\nBlocks \u00d7 NScratch\nFreeze\nPretrain\nAdaLNFigure 2: H-RDT framework. Our approach consists of two main stages: (1) pre-training on large-scale human manipulation\ndata with 48-dimensional hand pose representations, and (2) cross-embodiment fine-tuning with modular action encoders and\ndecoders adapted to specific robot action spaces.\nemploys flow matching (Lipman et al. 2022) for action gen-\neration, offering superior training stability and inference ef-\nficiency.\nStage 1: Human Data Pre-training In the first stage,\nwe train H-RDT on the complete EgoDex dataset with\n48-dimensional human hand action representations. Con-\ncretely, our model is trained on 338K+ trajectories across\n194 distinct manipulation tasks using the complete EgoDex\ndataset (Hoque et al. 2025), providing comprehensive cov-\nerage of human manipulation strategies, object interactions,\nand bimanual coordination patterns.\nStage 2: Cross-Embodiment Fine-tuning To quickly\nadapt pre-trained for cross-embodiment deployment, the\nsecond stage adapts the pre-trained model to specific robot\nembodiments through selective weight transfer and modu-\nlar re-initialization: The vision encoder, language encoder,\nand transformer backbone weights are transferred from the\npre-trained model, preserving learned multi-modal repre-\nsentations and manipulation priors developed from human\ndemonstrations. The state adaptor (MLP state), action adaptor\n(MLP action), and action decoder are completely reinitialized\nto handle the target robot\u2019s action space (e.g., 14 dimensions\nfor dual 7-DOF arms with parallel grippers)\nThis selective transfer strategy ensures that learned ma-\nnipulation semantics from human demonstrations are pre-\nserved while enabling adaptation to diverse robot morpholo-\ngies. The modular design allows action encoders and de-\ncoders to be retrained from scratch for each target embod-iment without compromising the learned visual-semantic\nrepresentations.\nH-RDT Architecture\nFlow Matching for Action Generation Rather than tra-\nditional diffusion training, H-RDT employs flow match-\ning (Lipman et al. 2022) for action generation, offering su-\nperior training stability and inference efficiency compared to\ntraditional diffusion modeling. Flow matching learns a vec-\ntor field that transforms a simple noise distribution to the\ntarget action distribution through a continuous normalizing\nflow.\nGiven a target action sequence a\u2217\nt:t+H, we construct a\nstraight-line flow path:\na\u03c4=\u03c4\u00b7a\u2217\nt:t+H+ (1\u2212\u03c4)z (1)\nwhere z\u223c N(0,I)is Gaussian noise and \u03c4\u2208[0,1]parame-\nterizes the flow time. The neural network v\u03b8learns to predict\nthe vector field:\nLFM=E\u03c4,z,a\u2217,c\u0002\n\u2225v\u03b8(a\u03c4, \u03c4,c)\u2212(z\u2212a\u2217\nt:t+H)\u22252\u0003\n(2)\nwhere c={ot,st,l}represents the conditioning informa-\ntion including multi-view RGB observations ot, propriocep-\ntionst, and language instruction l. During inference, we\nsample actions by integrating the learned vector field using\nan ODE solver with deterministic steps (detailed implemen-\ntation in Appendix B.3).\n\n--- Page 5 ---\nNetwork Architecture H-RDT adopts a unified trans-\nformer architecture comprising five modular components: a\nvision encoder, language encoder, modular action encoder,\ntransformer backbone, and modular action decoder.\nVision and Language Encoders: RGB observations are\nencoded using pre-trained visual backbones DinoV2 (Oquab\net al. 2023) and SigLIP (Zhai et al. 2023), followed by\nMLP adapters that project image features into the embed-\nding space of dimension dmodel. Text instructions are em-\nbedded using a pre-trained T5-XXL language model (Raffel\net al. 2020) and projected via similar adapters.\nModular Action Encoder: The proprioceptive state st\nand noisy action sequence a\u03c4\nt:t+Hare encoded with modular\nMLP adapters:\nhstate=StateAdapter (st)\u2208Rdmodel, (3)\nhaction=ActionAdapter (a\u03c4\nt:t+H)\u2208RH\u00d7dmodel,(4)\nwhere a\u03c4\nt:t+Hrepresents the noisy action sequence at flow\ntime\u03c4used in flow matching training, and Hdenotes the\nprediction horizon.\nTransformer Backbone: H-RDT adopts the LLaMA-3\narchitecture style (Touvron et al. 2023) with RMSNorm\nlayer normalization and SwiGLU activation functions. Each\ntransformer block processes the concatenated input x=\nConcat (hstate,haction)using self-attention, while image and\nlanguage features are injected via separate cross-attention to\navoid modality imbalance (Liu et al. 2024). The flow time\n\u03c4is mapped into timestep embeddings and integrated via\nAdaLN (Peebles and Xie 2023).\nModular Action Decoder: The predicted hidden states\nhaction are decoded using a modular MLP:\n\u02c6at:t+H=ActionDecoder (haction,temb). (5)\nwhere tembrepresents timestep embeddings for flow match-\ning, and the decoder outputs actions in the target robot\u2019s ac-\ntion space. The modular action encoder and decoder are re-\ninitialized during cross-embodiment fine-tuning.\nExperiments\nExperimental Setup\nWe conduct comprehensive experiments to evaluate H-\nRDT\u2019s effectiveness across simulation and real-world sce-\nnarios. Our evaluation covers four key dimensions: (1)\nsingle-task and multi-task performance across diverse ma-\nnipulation scenarios, (2) cross-embodiment generalization\nacross diverse robotic platforms, (3) environmental robust-\nness through domain randomization, and (4) sample effi-\nciency in few-shot learning with limited real-world demon-\nstrations.\nSimulation Environment: We use the RoboTwin 2.0\nplatform (Chen et al. 2025), a comprehensive dual-arm ma-\nnipulation benchmark featuring diverse household tasks.\nThe platform provides two evaluation modes: Easy mode\nwith clean tabletop environments, and Hard mode with do-\nmain randomization including 3cm table height variation,\nrandom backgrounds, lighting changes, and object clutter.\nRobot Embodiment: To demonstrate cross-embodiment\ntransfer capabilities, we evaluate H-RDT across multiplePerformance Level RDT Scratch H-RDT\n0.0: Complete failure - 7 -\n0.2: One fold single side - 18 -\n0.5: One fold both sides 3 - 3\n0.7: Two fold incomplete 12 - 9\n1.0: Two fold complete 10 - 13\nComplete success rate 40% 0% 52%\nTable 1: Towel folding task results on Aloha-Agilex-2.0\nplatform with detailed performance breakdown.\nrobotic platforms in both simulation and real-world settings.\nSimulation experiments cover two distinct embodiments:\nAloha-Agilex-1.0 and dual-arm Franka-Panda. Real-world\nvalidation uses three different platforms: dual-arm ARX5,\nAloha-Agilex-2.0 (dual-arm Piper), and UR5 + UMI config-\nuration.\nTraining Configuration: Detailed training configura-\ntions for different experimental settings are provided in Ap-\npendix C.\nBaselines and Comparison Methods\nWe compare H-RDT against several state-of-the-art meth-\nods:\n\u2022RDT (Liu et al. 2024): Robotics Diffusion Transformer\nbaseline\n\u2022\u03c00(Black et al. 2024): State-of-the-art vision-language-\naction model\n\u2022Scratch : Our model without human pre-training (train-\ning from scratch)\nReal-world Validation\nWe evaluate H-RDT across three distinct real-world robotic\nplatforms to validate cross-embodiment transfer capabilities\nand robustness in practical deployment scenarios. All real-\nworld experiments employ multi-task training.\nAloha-Agilex-2.0 Experiments We evaluate H-RDT on\nthe Aloha-Agilex-2.0 platform (dual-arm Piper) across two\nbimanual manipulation tasks.\nTask 1: Towel Folding This deformable object manipula-\ntion task tests the model\u2019s ability to handle non-rigid mate-\nrials through sequential folding operations.\nTask 2: Cup to Coaster Placement This spatial reasoning\ntask requires the model to automatically select the appropri-\nate hand based on object location: cups on the left side must\nbe grasped with the left hand, while cups on the right side\nmust be grasped with the right hand.\nBoth tasks employ sub-task-based scoring systems that\nevaluate progressive completion levels, with final evaluation\nfocusing on complete success rates. Tables 1 and 2 present\nthe performance breakdown across methods, with each task\nevaluated over 25 trials.\nFor the towel folding task (Table 1), H-RDT achieves a\n52% complete success rate compared to 40% for RDT and\n0% for training from scratch. The scratch model fails to\nachieve any complete folding, showing only partial success\n\n--- Page 6 ---\nPerformance Level RDT Scratch H-RDT\n0.0: Complete failure 10 14 6\n0.4: Grasped, failed to place 8 6 3\n1.0: Successfully completed 7 5 16\nComplete success rate 28% 20% 64%\nTable 2: Cup to coaster placement task results on Aloha-\nAgilex-2.0 platform with detailed performance breakdown.\nat lower skill levels, while RDT and H-RDT demonstrate\nmore sophisticated manipulation capabilities.\nThe cup-to-coaster task (Table 2) shows H-RDT achiev-\ning a 64% complete success rate compared to 28% for RDT\nand 20% for scratch training. H-RDT demonstrates the low-\nest failure rate and fewer instances of partial success, indi-\ncating more robust performance for tasks requiring spatial\nreasoning to select appropriate arms.\nOverall, H-RDT achieves an average success rate of 58%\nacross both bimanual tasks compared to 34% for RDT and\n10% for scratch training, demonstrating the effectiveness of\nhuman manipulation priors for diverse coordination chal-\nlenges including deformable object manipulation and spatial\nreasoning tasks.\nDual-arm ARX5 Few-shot Experiments To thoroughly\nvalidate the advantages of human manipulation priors, we\ndesign a challenging real-world experiment with a combina-\ntion of massive task diversity and data scarcity: 113 diverse\npick-and-place tasks using a dual-arm ARX5 robotic sys-\ntem, with only 1-5 demonstrations per task. This multi-task\nfew-shot setting is specifically designed to test the limits of\nsample efficiency and highlight the value of human behav-\nioral priors.\nThe EgoDex pretraining dataset contains extensive pick-\nand-place manipulation patterns similar to these tasks, pro-\nviding rich prior knowledge about how to perform such op-\nerations. Under these challenging conditions\u2014where even\nstate-of-the-art models like \u03c00struggle to properly fit the\nlimited demonstration trajectories\u2014H-RDT\u2019s human ma-\nnipulation priors enable noticeable performance improve-\nments. H-RDT achieves an average success rate of 41.6%\ncompared to 16.0% for RDT, 31.2% for \u03c00, and 17.6%\nfor Scratch, demonstrating the value of human manipulation\npriors for few-shot learning in data-limited scenarios.\nDual UR5 + UMI Experiments We evaluate H-RDT on\na dual UR5 robotic system with demonstrations collected\nusing Universal Manipulation Interface (UMI) (Chi et al.\n2024), a data collection framework that enables portable,\nlow-cost human demonstration collection through hand-held\ngrippers.\nThe evaluation focuses on bimanual takeout bag place-\nment tasks decomposed into four sequential subtasks: right-\nhand pick, right-hand place, left-hand pick, and left-hand\nplace.\nTable 4 presents the success rates for each subtask across\ndifferent methods, with each evaluation conducted over 25\ntrials.\nH-RDT achieves consistently superior performanceacross all subtasks, with an average success rate of 58.0%\ncompared to 29.0% for RDT, 31.0% for \u03c00, and 16.0%\nfor training from scratch. The results show notable im-\nprovements in pick operations (64% and 60% for right and\nleft hands, respectively) and 27-42% absolute improvements\nover baseline methods, demonstrating the value of human\nmanipulation priors for bimanual coordination.\nSimulation Results on RoboTwin 2.0\nSingle-Task Performance We evaluate single-task per-\nformance on 13 representative manipulation tasks from\nthe RoboTwin 2.0 benchmark. Each task is trained on 50\ndemonstrations collected in clean environments and evalu-\nated in two modes: Easy mode (clean tabletop environments\nmatching training conditions) and Hard mode (challenging\nenvironments with domain randomization including lighting\nchanges, object clutter, and table height variations). Detailed\nresults for all tasks are provided in Table 8 in the Appendix.\nH-RDT achieves the highest average success rate of\n68.7% in Easy mode and 25.6% in Hard mode, demonstrat-\ning significant improvements over existing methods. H-RDT\nsubstantially surpasses training from scratch (Scratch) by\n8.4% in both Easy and Hard modes, validating the effec-\ntiveness of human manipulation pre-training.\nMulti-Task Performance We conduct multi-task experi-\nments on 45 tasks from RoboTwin 2.0, training on approx-\nimately 2250 demonstrations collected under domain ran-\ndomization (Hard mode data). Table 5 shows the results on\na representative subset of 10 tasks evaluated in Hard mode.\nIn multi-task settings, H-RDT achieves an average\nsuccess rate of 87.2%, significantly outperforming RDT\n(28.8%), \u03c00(48.4%), and Scratch (67.2%). H-RDT shows a\nsubstantial 20.0% absolute improvement over training from\nscratch, which is notably larger than the improvements ob-\nserved in single-task scenarios, demonstrating that human\nmanipulation pre-training provides even greater advantages\nwhen learning across diverse tasks simultaneously.\nCross-Embodiment Generalization To further validate\nthe cross-embodiment transfer capabilities of H-RDT, we\nconduct multi-task experiments across two different robotic\nembodiments in simulation. We evaluate both Aloha-\nAgilex-1.0 and Franka-Panda platforms using the same ex-\nperimental setup as described above. Figure 3 shows the per-\nformance comparison across these platforms.\nH-RDT demonstrates strong performance across both\nembodiments, achieving 87.2% on Aloha-Agilex-1.0 and\n62.9% on Franka-Panda, significantly outperforming base-\nline methods on both platforms. Detailed per-task results for\nFranka-Panda are provided in Table 6 in the Appendix. The\nconsistent improvements across different robotic morpholo-\ngies validate the cross-embodiment generalization capabili-\nties of our modular action encoder design.\nAnalysis and Discussion\nImpact of Human Pre-training: The consistent improve-\nments of H-RDT over Scratch(H-RDT w/o human) across\nall experimental settings validate our core hypothesis that\n\n--- Page 7 ---\nTask Category RDT \u03c00 Scratch H-RDT\nPick yellow item to the plate 12% 16% 28% 40%\nPlace banana and carrot in basket 24% 40% 8% 44%\nStack two bowls together 32% 60% 40% 68%\nPlace cube in front of highest chips 12% 0% 0% 20%\nStack one can on top of the other can 0% 40% 12% 36%\nAverage 16.0% 31.2% 17.6% 41.6%\nTable 3: Real-world few-shot learning results on manipulation tasks. Each task has 1-5 demonstrations for training. H-RDT\ndemonstrates superior sample efficiency and real-world transfer capabilities.\nSubtask RDT \u03c00 Scratch H-RDT\nRight hand pick 36% 40% 20% 64%\nRight hand place 32% 28% 16% 56%\nLeft hand pick 28% 36% 20% 60%\nLeft hand place 20% 20% 8% 52%\nAverage 29.0% 31.0% 16.0% 58.0%\nTable 4: Performance breakdown on dual UR5 + UMI take-\nout bag placement task showing success rates for individual\nsubtasks.\nTask RDT \u03c00 Scratch H-RDT\nClick Alarmclock 30% 69% 69% 94%\nClick Bell 13% 40% 77% 98%\nDump Bin Bigbin 22% 33% 58% 89%\nGrab Roller 57% 71% 91% 97%\nHandover Mic 81% 97% 93% 99%\nMove Playingcard Away 3% 23% 44% 67%\nOpen Laptop 33% 35% 88% 92%\nOpen Microwave 19% 46% 32% 82%\nPress Stapler 15% 59% 71% 81%\nStack Bowls Three 15% 11% 49% 73%\nAverage 28.8% 48.4% 67.2% 87.2%\nTable 5: Multi-task success rates (%) on RoboTwin 2.0\nbenchmark (Hard mode evaluation). Results show perfor-\nmance after multi-task training on 50 tasks with domain ran-\ndomized data.\nAloha-Agilex-1.0 Franka-Panda020406080100Success Rate (%)28.8%48.4%67.2%87.2%+20.0%\n23.1%32.0%44.0%62.9%+18.9%RDT\nPi0\nScratch\nH-RDT\nFigure 3: Cross-embodiment multi-task performance on\nRoboTwin 2.0 tasks.human manipulation data provides valuable inductive bi-\nases. The benefits are most pronounced in few-shot real-\nworld scenarios, where human priors about object affor-\ndances and manipulation strategies prove crucial.\nEnvironmental Robustness: H-RDT demonstrates strong\nperformance under challenging conditions in RoboTwin 2.0\nHard mode with domain randomization. The model suc-\ncessfully handles environmental variations including light-\ning changes, object clutter, and table height variations, con-\nsistently outperforming baseline methods.\nSample Efficiency: In few-shot real-world experiments, H-\nRDT\u2019s ability to learn from just 1-5 demonstrations per task\nsignificantly outperforms baselines, highlighting the practi-\ncal value of human behavior priors for reducing data require-\nments in robotic learning.\nTask Diversity and Complexity: Real-world experiments\ndemonstrate H-RDT\u2019s capability to handle diverse manipu-\nlation challenges, including deformable object manipulation\nand tasks requiring spatial reasoning, showcasing its versa-\ntility across different manipulation complexities.\nCross-Platform Robustness: Our comprehensive evalua-\ntion across both simulation and real-world settings demon-\nstrates H-RDT\u2019s robust performance across diverse robotic\nembodiments, including Aloha-Agilex-1.0, dual-arm Piper,\ndual-arm ARX5, dual-arm Franka-Panda, and dual-arm\nUR5+UMI platforms. This cross-platform consistency val-\nidates the effectiveness of our modular architecture design\nand human-to-robot knowledge transfer approach.\nConclusion\nThis paper introduces H-RDT, a novel approach that lever-\nages large-scale egocentric human manipulation videos with\n3D hand pose annotations to enhance robotic manipulation\ncapabilities. Our central contribution shows that rich manip-\nulation knowledge can be learned from human behavioral\npriors and adapted to diverse robotic manipulation tasks.\nThe key technical innovations include: (1) a modular\ntransformer architecture with specialized action encoders\nand decoders that enable cross-embodiment adaptation, (2)\nflow matching for stable and efficient policy learning, and\n(3) a two-stage training paradigm that first pre-trains on hu-\nman data before fine-tuning on robot-specific data.\nComprehensive evaluations across both simulation and\nreal-world scenarios demonstrate consistent improvements\nover state-of-the-art methods, validating that human manip-\nulation priors provide powerful inductive biases for sample-\nefficient robotic learning.\n\n--- Page 8 ---\nReferences\nAldaco, J.; Armstrong, T.; Baruch, R.; Bingham, J.; Chan,\nS.; Draper, K.; Dwibedi, D.; Finn, C.; Florence, P.;\nGoodrich, S.; et al. 2024. Aloha 2: An enhanced low-\ncost hardware for bimanual teleoperation. arXiv preprint\narXiv:2405.02292 .\nBanerjee, P.; Shkodrani, S.; Moulon, P.; Hampali, S.; Han,\nS.; Zhang, F.; Zhang, L.; Fountain, J.; Miller, E.; Basol, S.;\net al. 2025. Hot3d: Hand and object tracking in 3d from ego-\ncentric multi-view videos. In Proceedings of the Computer\nVision and Pattern Recognition Conference , 7061\u20137071.\nBao, F.; Nie, S.; Xue, K.; Cao, Y .; Li, C.; Su, H.; and Zhu,\nJ. 2023. All are worth words: A vit backbone for diffu-\nsion models. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 22669\u201322679.\nBlack, K.; Brown, N.; Driess, D.; Esmail, A.; Equi, M.; Finn,\nC.; Fusai, N.; Groom, L.; Hausman, K.; Ichter, B.; et al.\n2024. \u03c00: A Vision-Language-Action Flow Model for Gen-\neral Robot Control. arXiv preprint arXiv:2410.24164 .\nBrohan, A.; Brown, N.; Carbajal, J.; Chebotar, Y .; Chen,\nX.; Choromanski, K.; Ding, T.; Driess, D.; Dubey, A.;\nFinn, C.; et al. 2023. Rt-2: Vision-language-action models\ntransfer web knowledge to robotic control. arXiv preprint\narXiv:2307.15818 .\nBu, Q.; Cai, J.; Chen, L.; Cui, X.; Ding, Y .; Feng, S.;\nGao, S.; He, X.; Hu, X.; Huang, X.; et al. 2025. Agi-\nbot world colosseo: A large-scale manipulation platform for\nscalable and intelligent embodied systems. arXiv preprint\narXiv:2503.06669 .\nChen, T.; Chen, Z.; Chen, B.; Cai, Z.; Liu, Y .; Liang, Q.;\nLi, Z.; Lin, X.; Ge, Y .; Gu, Z.; et al. 2025. RoboTwin 2.0:\nA Scalable Data Generator and Benchmark with Strong Do-\nmain Randomization for Robust Bimanual Robotic Manip-\nulation. arXiv preprint arXiv:2506.18088 .\nChi, C.; Xu, Z.; Feng, S.; Cousineau, E.; Du, Y .; Burchfiel,\nB.; Tedrake, R.; and Song, S. 2023. Diffusion policy: Visuo-\nmotor policy learning via action diffusion. The International\nJournal of Robotics Research , 02783649241273668.\nChi, C.; Xu, Z.; Pan, C.; Cousineau, E.; Burchfiel, B.; Feng,\nS.; Tedrake, R.; and Song, S. 2024. Universal manipu-\nlation interface: In-the-wild robot teaching without in-the-\nwild robots. arXiv preprint arXiv:2402.10329 .\nDamen, D.; Doughty, H.; Farinella, G. M.; Fidler, S.;\nFurnari, A.; Kazakos, E.; Moltisanti, D.; Munro, J.; Perrett,\nT.; Price, W.; et al. 2018. Scaling egocentric vision: The\nepic-kitchens dataset. In Proceedings of the European con-\nference on computer vision (ECCV) , 720\u2013736.\nEsser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; M \u00a8uller, J.;\nSaini, H.; Levi, Y .; Lorenz, D.; Sauer, A.; Boesel, F.; et al.\n2024. Scaling rectified flow transformers for high-resolution\nimage synthesis. In Forty-first international conference on\nmachine learning .\nFang, H.-S.; Fang, H.; Tang, Z.; Liu, J.; Wang, C.; Wang, J.;\nZhu, H.; and Lu, C. 2023. Rh20t: A comprehensive robotic\ndataset for learning diverse skills in one-shot. arXiv preprint\narXiv:2307.00595 .Grauman, K.; Westbury, A.; Byrne, E.; Chavis, Z.; Furnari,\nA.; Girdhar, R.; Hamburger, J.; Jiang, H.; Liu, M.; Liu, X.;\net al. 2022. Ego4d: Around the world in 3,000 hours of ego-\ncentric video. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition , 18995\u201319012.\nGrauman, K.; Westbury, A.; Torresani, L.; Kitani, K.; Malik,\nJ.; Afouras, T.; Ashutosh, K.; Baiyya, V .; Bansal, S.; Boote,\nB.; et al. 2024. Ego-exo4d: Understanding skilled human ac-\ntivity from first-and third-person perspectives. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , 19383\u201319400.\nHoque, R.; Huang, P.; Yoon, D. J.; Sivapurapu, M.; and\nZhang, J. 2025. EgoDex: Learning Dexterous Manipula-\ntion from Large-Scale Egocentric Video. arXiv preprint\narXiv:2505.11709 .\nIntelligence, P.; Black, K.; Brown, N.; Darpinian, J.; Dha-\nbalia, K.; Driess, D.; Esmail, A.; Equi, M.; Finn, C.; Fu-\nsai, N.; et al. 2025. \u03c00.5: A Vision-Language-Action\nModel with Open-World Generalization. arXiv preprint\narXiv:2504.16054 .\nKareer, S.; Patel, D.; Punamiya, R.; Mathur, P.; Cheng, S.;\nWang, C.; Hoffman, J.; and Xu, D. 2024. Egomimic: Scal-\ning imitation learning via egocentric video. arXiv preprint\narXiv:2410.24221 .\nKhazatsky, A.; Pertsch, K.; Nair, S.; Balakrishna, A.;\nDasari, S.; Karamcheti, S.; Nasiriany, S.; Srirama, M. K.;\nChen, L. Y .; Ellis, K.; et al. 2024. Droid: A large-scale\nin-the-wild robot manipulation dataset. arXiv preprint\narXiv:2403.12945 .\nKim, M. J.; Pertsch, K.; Karamcheti, S.; Xiao, T.; Balakr-\nishna, A.; Nair, S.; Rafailov, R.; Foster, E.; Lam, G.; Sanketi,\nP.; et al. 2024. Openvla: An open-source vision-language-\naction model. arXiv preprint arXiv:2406.09246 .\nLi, Z.; Wang, J.; Chen, H.; Liu, M.; et al. 2025. H2R:\nLearning Human-to-Robot Imitation with Data Augmenta-\ntion. arXiv preprint arXiv:2501.xxxxx .\nLipman, Y .; Chen, R. T.; Ben-Hamu, H.; Nickel, M.; and\nLe, M. 2022. Flow matching for generative modeling. arXiv\npreprint arXiv:2210.02747 .\nLiu, J.; Chen, H.; An, P.; Liu, Z.; Zhang, R.; Gu, C.; Li, X.;\nGuo, Z.; Chen, S.; Liu, M.; et al. 2025. Hybridvla: Col-\nlaborative diffusion and autoregression in a unified vision-\nlanguage-action model. arXiv preprint arXiv:2503.10631 .\nLiu, Q. 2022. Rectified flow: A marginal preserv-\ning approach to optimal transport. arXiv preprint\narXiv:2209.14577 .\nLiu, S.; Wu, L.; Li, B.; Tan, H.; Chen, H.; Wang, Z.; Xu,\nK.; Su, H.; and Zhu, J. 2024. Rdt-1b: a diffusion foun-\ndation model for bimanual manipulation. arXiv preprint\narXiv:2410.07864 .\nLiu, Y .; Liu, Y .; Jiang, C.; Lyu, K.; Wan, W.; Shen, H.; Liang,\nB.; Fu, Z.; Wang, H.; and Yi, L. 2022. Hoi4d: A 4d egocen-\ntric dataset for category-level human-object interaction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition , 21013\u201321022.\n\n--- Page 9 ---\nOquab, M.; Darcet, T.; Moutakanni, T.; V o, H.; Szafraniec,\nM.; Khalidov, V .; Fernandez, P.; Haziza, D.; Massa, F.; El-\nNouby, A.; et al. 2023. Dinov2: Learning robust visual fea-\ntures without supervision. arXiv preprint arXiv:2304.07193 .\nO\u2019Neill, A.; Rehman, A.; Maddukuri, A.; Gupta, A.;\nPadalkar, A.; Lee, A.; Pooley, A.; Gupta, A.; Mandlekar, A.;\nJain, A.; et al. 2024. Open x-embodiment: Robotic learning\ndatasets and rt-x models: Open x-embodiment collaboration\n0. In 2024 IEEE International Conference on Robotics and\nAutomation (ICRA) , 6892\u20136903. IEEE.\nPeebles, W.; and Xie, S. 2023. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision , 4195\u20134205.\nQiu, R.-Z.; Yang, S.; Cheng, X.; Chawla, C.; Li, J.; He,\nT.; Yan, G.; Yoon, D. J.; Hoque, R.; Paulsen, L.; et al.\n2025. Humanoid Policy\u02dc Human Policy. arXiv preprint\narXiv:2503.13441 .\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-\ning the limits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research , 21(140):\n1\u201367.\nTeam, O. M.; Ghosh, D.; Walke, H.; Pertsch, K.; Black, K.;\nMees, O.; Dasari, S.; Hejna, J.; Kreiman, T.; Xu, C.; et al.\n2024. Octo: An open-source generalist robot policy. arXiv\npreprint arXiv:2405.12213 .\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971 .\nWang, C.; Shi, H.; Wang, W.; Zhang, R.; Fei-Fei, L.; and\nLiu, C. K. 2024. Dexcap: Scalable and portable mocap data\ncollection system for dexterous manipulation. arXiv preprint\narXiv:2403.07788 .\nWen, C.; Lin, X.; Wei, J. S.; Chen, K.; and Gao, Y . 2024.\nAny-point trajectory modeling for policy learning. arXiv\npreprint arXiv:2401.00025 .\nWen, J.; Zhu, Y .; Li, J.; Tang, Z.; Shen, C.; and Feng, F.\n2025a. DexVLA: Vision-Language Model with Plug-In Dif-\nfusion Expert for General Robot Control. arXiv preprint\narXiv:2502.05855 .\nWen, J.; Zhu, Y .; Li, J.; Zhu, M.; Tang, Z.; Wu, K.; Xu, Z.;\nLiu, N.; Cheng, R.; Shen, C.; et al. 2025b. Tinyvla: Towards\nfast, data-efficient vision-language-action models for robotic\nmanipulation. IEEE Robotics and Automation Letters .\nWu, K.; Hou, C.; Liu, J.; Che, Z.; Ju, X.; Yang, Z.; Li, M.;\nZhao, Y .; Xu, Z.; Yang, G.; et al. 2024. Robomind: Bench-\nmark on multi-embodiment intelligence normative data for\nrobot manipulation. arXiv preprint arXiv:2412.13877 .\nXie, S.; Cao, H.; Weng, Z.; Xing, Z.; Shen, S.; Leng, J.; Qiu,\nX.; Fu, Y .; Wu, Z.; and Jiang, Y .-G. 2025. Human2Robot:\nLearning Robot Actions from Paired Human-Robot Videos.\narXiv preprint arXiv:2502.16587 .\nXu, M.; Zhang, H.; Hou, Y .; Xu, Z.; Fan, L.; Veloso, M.; and\nSong, S. 2025. DexUMI: Using Human Hand as the Uni-\nversal Manipulation Interface for Dexterous Manipulation.\narXiv preprint arXiv:2505.21864 .Xu, M.; Zhang, Z.; Chi, C.; and Song, S. 2024. Flow as\nthe Cross-Domain Manipulation Interface. arXiv preprint\narXiv:2407.15208 .\nZe, Y .; Zhang, G.; Zhang, K.; Hu, C.; Wang, M.; and Xu, H.\n2024. 3d diffusion policy. arXiv e-prints , arXiv\u20132403.\nZhai, X.; Mustafa, B.; Kolesnikov, A.; and Beyer, L. 2023.\nSigmoid loss for language image pre-training. arXiv\npreprint arXiv:2303.15343 .\nZhao, Q.; Lu, Y .; Kim, M. J.; Fu, Z.; Zhang, Z.; Wu, Y .;\nLi, Z.; Ma, Q.; Han, S.; Finn, C.; et al. 2025. Cot-vla: Vi-\nsual chain-of-thought reasoning for vision-language-action\nmodels. In Proceedings of the Computer Vision and Pattern\nRecognition Conference , 1702\u20131713.\nZhao, T. Z.; Kumar, V .; Levine, S.; and Finn, C. 2023. Learn-\ning fine-grained bimanual manipulation with low-cost hard-\nware. arXiv preprint arXiv:2304.13705 .\nZhen, H.; Qiu, X.; Chen, P.; Yang, J.; Yan, X.; Du, Y .; Hong,\nY .; and Gan, C. 2024. 3d-vla: A 3d vision-language-action\ngenerative world model. arXiv preprint arXiv:2403.09631 .\n\n--- Page 10 ---\nA. Additional Experimental Results\nA.1 Single-Task Performance Results (RoboTwin\n2.0 Benchmark)\nThis section provides comprehensive per-task results for 13\nmanipulation tasks evaluated in the single-task experiments\non the RoboTwin 2.0 benchmark. Table 8 presents the de-\ntailed success rates across all baseline methods in both Easy\nand Hard evaluation modes.\nTo accelerate training speed in single-task experiments,\nwe concatenate three 240\u00d7320 views into a single 360\u00d7320\ninput for training, which may result in some performance\ndegradation compared to higher resolution settings.\nA.2 Franka-Panda Detailed Results\nThis section provides comprehensive per-task results for the\nFranka-Panda embodiment on the 10-task subset used for\ndetailed evaluation in the main paper.\nTask RDT \u03c00 Scratch H-RDT\nClick Alarmclock 58% 71% 59% 94%\nClick Bell 18% 48% 39% 89%\nDump Bin Bigbin 8% 13% 21% 31%\nGrab Roller 50% 35% 70% 75%\nHandover Mic 13% 17% 53% 64%\nMove Playingcard Away 3% 7% 17% 20%\nOpen Laptop 15% 11% 41% 43%\nOpen Microwave 1% 45% 29% 59%\nPress Stapler 34% 42% 51% 86%\nShake Bottle 31% 31% 60% 68%\nAverage 23.1% 32.0% 44.0% 62.9%\nTable 6: Detailed Franka-Panda multi-task results (%) on 10-\ntask subset.\nB. Implementation Details\nB.1 Model Architecture\nTable 7 provides the key hyperparameter settings for the H-\nRDT model architecture.\nB.2 Training Configuration and Data Processing\nTraining Configuration: Both pre-training and fine-tuning\nuse AdamW optimizer with learning rate 1e-4, weight de-\ncay 0.01, and gradient clipping at norm 1.0. We employ\nmixed-precision training (bfloat16) for computational effi-\nciency and gradient accumulation to maintain effective batch\nsizes.\nData Processing: Images are processed with 196 patches\nfor vision encoding. Language instructions are tokenized\nwith a maximum length of 1024 tokens. During pre-training,\nwe process the full 338K+ trajectory EgoDex dataset with\n48-dimensional human hand actions. Fine-tuning adapts to\ntarget robot action spaces.\nB.3 Details on Flow Matching\nTraining Implementation: We employ uniform timestep\nsampling with \u03c4\u2208[0,0.999] during training.Component Configuration\nTransformer Backbone\nHidden Size 2176\nLayers 16\nAttention Heads 16\nKey-Value Heads (GQA) 8\nLayer Norm Epsilon 1e-5\nActivation Function SwiGLU\nAction Dimensions\nHuman Hand Pose (Pretrain) 48\nDual 6-DOF Arms (Finetune) 14\nCamera Views\nPretrain 1 (egocentric)\nFinetune 1-3 (robot-dependent)\nAdapter Networks\nState/Action Adaptor 3-layer MLP + SiLU\nImage/Language Adaptor 2-layer MLP + SiLU\nFlow Matching\nInference Steps 5\nTimestep Range [0, 0.999]\nSampling Strategy Uniform\nModel Scale\nTotal Parameters 2B\nTable 7: H-RDT architecture hyperparameters and key con-\nfiguration settings.\nInference Implementation: During inference, we start with\nGaussian noise a0\u223c N(0,I)and integrate the learned vec-\ntor field using a deterministic ODE solver with 5 function\nevaluations and step size \u2206t= 1/5 = 0 .2. At each step,\nwe update the action as at+\u2206t=at+ \u2206t\u00b7v\u03b8(at, t,c)for\nreal-time performance (30Hz control frequency).\nB.4 Real-World Task Definitions\nFigure 4 provides visual illustrations of the real-world ma-\nnipulation task in our experiments.\nC. Training Configurations\nThis section provides detailed training configurations for all\nexperimental settings described in the main paper.\nC.1 Simulation Experiments\nC.1.1 Single-Task Configuration\n\u2022Data: 13 tasks, 50 clean trajectories per task\n\u2022Platform: Aloha-Agilex-1.0\n\u2022Training: 10k steps, 4 H100 GPUs, batch size 16 per\nGPU\n\u2022Evaluation: Easy mode (clean scenes) and Hard mode\n(domain randomization)\nC.1.2 Multi-Task Configuration\n\u2022Data: 45 tasks, each 50 domain-randomized trajectories\n\u2022Platform: Aloha-Agilex-1.0, Franka-Panda\n\u2022Training: 30k steps for Aloha, 10k steps for Franka, 4\nH100 GPUs, batch size 32 per GPU\n\u2022Evaluation: Hard mode with domain randomization,\nthree camera views\n\n--- Page 11 ---\nTaskACT DP RDT \u03c00 Scratch H-RDT\nEasy Hard Easy Hard Easy Hard Easy Hard Easy Hard Easy Hard\nGrab Roller 66% 6% 98% 1% 74% 43% 96% 80% 99% 37% 95% 63%\nHandover Mic 9% 0% 53% 0% 98% 41% 100% 13% 100% 4% 100% 14%\nLift Pot 7% 2% 37% 0% 82% 19% 84% 36% 97% 11% 94% 27%\nMove Can Pot 0% 0% 42% 0% 47% 23% 74% 32% 63% 5% 54% 26%\nOpen Laptop 32% 0% 46% 0% 61% 36% 85% 46% 91% 59% 92% 42%\nPick Dual Bottles 4% 0% 26% 0% 42% 13% 57% 12% 37% 16% 67% 20%\nPlace Object Basket 0% 0% 17% 0% 42% 14% 62% 10% 52% 15% 62% 19%\nPlace Dual Shoes 0% 0% 7% 0% 4% 4% 15% 0% 14% 4% 32% 9%\nPlace Phone Stand 0% 0% 17% 0% 15% 6% 35% 7% 25% 2% 36% 9%\nPut Bottles Dustbin 0% 0% 23% 0% 21% 4% 54% 13% 53% 14% 64% 16%\nPut Object Cabinet 4% 18% 50% 17% 30% 30% 69% 29% 58% 41% 63% 37%\nStack Blocks Two 0% 0% 6% 0% 32% 1% 40% 1% 28% 3% 50% 3%\nStack Bowls Two 0% 0% 34% 0% 73% 21% 73% 41% 67% 13% 84% 48%\nAverage 9.4% 2.0% 35.1% 1.4% 47.8% 19.6% 64.9% 24.6% 60.3% 17.2% 68.7% 25.6%\nTable 8: Single-task success rates (%) on RoboTwin 2.0 benchmark. H-RDT achieves the highest average performance across\nboth Easy and Hard evaluation modes. Bold indicates the best performance for each task and mode.\nC.2 Real-World Experiments\nC.2.1 Dual-arm ARX5\n\u2022Data: Few-shot pick-place tasks with 1-5 trajectories per\ntask, 607 trajectories total across 113 tasks\n\u2022Platform: Dual-arm ARX5\n\u2022Training: 100k steps, 4 H100 GPUs\nC.2.2 Dual-arm UR5+UMI Platform\n\u2022Data: Dual-hand takeout bag placement tasks, 100 tra-\njectories\n\u2022Platform: Dual-arm UR5+UMI Platform\n\u2022Training: 20k steps, 8 H100 GPUs\nC.2.3 Aloha-Agilex-2.0 Platform\n\u2022Data: Pouring water and folding clothes, each 50 trajec-\ntories\n\u2022Platform: Aloha-Agilex-2.0 (dual-arm Piper)\n\u2022Training: 50k steps, 8 H20 GPUs\n\n--- Page 12 ---\nTowel Folding:  Initially a towel is randomly placed on the table. Each robot arm grasps the bottom edges of the towel on both sides and pulls\ndownward to straighten it (#1). The arms then perform a one-fold by bringing the edges together (#2). Finally , the right arm grasps the right edge\nand performs a second fold to complete  the two -fold towel folding (#3).Init. #1 #2 #3 End.\nLeft Init.\n Left #1\n Right Init.\nCup to Coaster  Placement:   Initially a cup and a coaster are randomly placed on the table. The robot first performs spatial reasoning to\ndetermine which arm should be used: if the cup is positioned on the left side, the left arm grasps it; if the cup is on the right side, the right arm\ngrasps it (#1). The selected arm then precisely places the cup onto the coaster .Dual-Arm Piper\nDual-Arm UR5\nDual-Arm ARX5\nRight #1\nTakeout Bag Place ment:  Initially two takeout bags are placed on the left and right sides of a basket. The task involves bimanual coordination\nthrough four sequential subtasks: the left arm first picks up a takeout bag (#1), then places it into the basket (#2), followed by the right arm\npicking up the other takeout bag (#3) and placing it into the basket  (#4).\nInit.\n #1\n #2\n #3\nPick yellow item to the\nplatePlace banana and\ncarrot in basketPlace cube in fr ont of\nhighest chipsStack one can on top of\nthe other  can Stack two bowls\ntogether#4End.Figure 4: Task definition of real-world experiments.",
  "project_dir": "artifacts/projects/enhanced_cs.RO_2507.23523v1_H_RDT_Human_Manipulation_Enhanced_Bimanual_Roboti",
  "communication_dir": "artifacts/projects/enhanced_cs.RO_2507.23523v1_H_RDT_Human_Manipulation_Enhanced_Bimanual_Roboti/.agent_comm",
  "assigned_at": "2025-08-03T20:49:27.183058",
  "status": "assigned"
}